{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192b5c22194d47cbaca4d8548ae32c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2989, 'grad_norm': 0.4476770758628845, 'learning_rate': 4.5e-05, 'epoch': 0.5}\n",
      "{'loss': 3.0744, 'grad_norm': 1.0051884651184082, 'learning_rate': 4e-05, 'epoch': 1.0}\n",
      "{'loss': 3.172, 'grad_norm': 0.2706291675567627, 'learning_rate': 3.5e-05, 'epoch': 1.5}\n",
      "{'loss': 3.4702, 'grad_norm': 0.605918824672699, 'learning_rate': 3e-05, 'epoch': 2.0}\n",
      "{'loss': 3.2345, 'grad_norm': 0.32159414887428284, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n",
      "{'loss': 3.0475, 'grad_norm': 0.2641453742980957, 'learning_rate': 2e-05, 'epoch': 3.0}\n",
      "{'loss': 3.22, 'grad_norm': 0.5491503477096558, 'learning_rate': 1.5e-05, 'epoch': 3.5}\n",
      "{'loss': 3.6996, 'grad_norm': 0.4256013333797455, 'learning_rate': 1e-05, 'epoch': 4.0}\n",
      "{'loss': 3.2737, 'grad_norm': 0.379459947347641, 'learning_rate': 5e-06, 'epoch': 4.5}\n",
      "{'loss': 3.1594, 'grad_norm': 0.2503287196159363, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 1.826, 'train_samples_per_second': 16.429, 'train_steps_per_second': 5.476, 'train_loss': 3.265015172958374, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Suppress warnings (optional, for cleaner output)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    clean_up_tokenization_spaces=True  # Explicitly set to suppress FutureWarning\n",
    ")\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Inspect the model architecture to find the correct layer names for LoRA\n",
    "# Print the model to see its structure (uncomment to debug)\n",
    "# print(model)\n",
    "\n",
    "# Define the LoRA configuration with corrected target modules\n",
    "# For GPT-2, the attention layers are typically in transformer.h[i].attn\n",
    "# After inspecting GPT-2's architecture, the correct target module for attention weights is often \"attn.c_attn\"\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"h.0.attn.c_attn\", \"h.1.attn.c_attn\"],  # Adjusted for GPT-2's naming convention\n",
    ")\n",
    "\n",
    "# Apply the PEFT model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Expanded reasoning dataset (still small, but more diverse)\n",
    "train_data = [\n",
    "    \"Q: What is 2 + 2? Let's think step by step. First, we know that 2 is a number. Adding another 2 to it gives us 4.\",\n",
    "    \"Q: What is the capital of France? Let's think step by step. France is a country in Europe. The capital of France is Paris.\",\n",
    "    \"Q: If a train leaves the station at 3 PM and travels at 60 mph, what time will it reach a station 180 miles away? Let's think step by step. The train travels 60 miles in one hour. To travel 180 miles, it will take 3 hours. Therefore, it will reach the station at 6 PM.\",\n",
    "    \"Q: What is 5 + 3? Let's think step by step. Starting with 5, adding 3 more gives us 8.\",\n",
    "    \"Q: What is the capital of Japan? Let's think step by step. Japan is an island nation in Asia. Its capital is Tokyo.\",\n",
    "    \"Q: If a car drives at 40 mph for 120 miles, how long does it take? Let's think step by step. The car travels 40 miles in one hour. For 120 miles, it takes 120 / 40 = 3 hours.\"\n",
    "]\n",
    "\n",
    "# Custom dataset class\n",
    "class ReasoningDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(\n",
    "            self.data[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()  # Add labels for causal LM\n",
    "        return inputs\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = ReasoningDataset(train_data, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,  # Increased epochs for better training\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,  # Log more frequently to monitor progress\n",
    "    disable_tqdm=False  # Ensure progress bar is enabled (if supported)\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Trainer for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "peft_model.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Q: What is 2 + 2? Let's think step by step.\n",
      "Response: Q: What is 2 + 2? Let's think step by step.\n",
      "\n",
      "1.1.1:\n",
      "\n",
      "1.2.1:\n",
      "\n",
      "When I think of a time I need to spend thinking about things, I think about\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 載入分詞器和模型\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 確保填充標記設置\n",
    "\n",
    "# 載入微調後的模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# 設置模型為評估模式\n",
    "model.eval()\n",
    "\n",
    "# 定義推理函數\n",
    "def generate_response(prompt, max_length=50):\n",
    "    # 將輸入轉換為模型可用的格式\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # 生成輸出\n",
    "    with torch.no_grad():  # 禁用梯度計算以節省記憶體\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,  # 使用隨機採樣生成（可選）\n",
    "            temperature=0.7,  # 控制生成的多樣性（可調整）\n",
    "            top_k=50,  # 控制生成時的候選詞數（可調整）\n",
    "        )\n",
    "    \n",
    "    # 解碼輸出\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# 測試推理\n",
    "prompt = \"Q: What is 2 + 2? Let's think step by step.\"\n",
    "response = generate_response(prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844aab517f064ce396cdbc91c7415526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2861, 'grad_norm': 0.3855910301208496, 'learning_rate': 0.00019, 'epoch': 0.5}\n",
      "{'loss': 3.2555, 'grad_norm': 0.7794333696365356, 'learning_rate': 0.00018, 'epoch': 1.0}\n",
      "{'loss': 3.2531, 'grad_norm': 0.437178373336792, 'learning_rate': 0.00017, 'epoch': 1.5}\n",
      "{'loss': 3.4181, 'grad_norm': 0.5153894424438477, 'learning_rate': 0.00016, 'epoch': 2.0}\n",
      "{'loss': 3.3429, 'grad_norm': 0.4430333375930786, 'learning_rate': 0.00015000000000000001, 'epoch': 2.5}\n",
      "{'loss': 3.1305, 'grad_norm': 0.5874897837638855, 'learning_rate': 0.00014, 'epoch': 3.0}\n",
      "{'loss': 3.1028, 'grad_norm': 0.3364436626434326, 'learning_rate': 0.00013000000000000002, 'epoch': 3.5}\n",
      "{'loss': 3.6601, 'grad_norm': 0.6103304624557495, 'learning_rate': 0.00012, 'epoch': 4.0}\n",
      "{'loss': 3.211, 'grad_norm': 0.46137022972106934, 'learning_rate': 0.00011000000000000002, 'epoch': 4.5}\n",
      "{'loss': 3.1998, 'grad_norm': 0.6468428373336792, 'learning_rate': 0.0001, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed73d20f0a7467d997b3cce4c8c44d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.390946865081787, 'eval_runtime': 0.039, 'eval_samples_per_second': 51.277, 'eval_steps_per_second': 25.638, 'epoch': 5.0}\n",
      "{'loss': 3.0529, 'grad_norm': 0.5617461204528809, 'learning_rate': 9e-05, 'epoch': 5.5}\n",
      "{'loss': 3.632, 'grad_norm': 1.2615066766738892, 'learning_rate': 8e-05, 'epoch': 6.0}\n",
      "{'loss': 3.2693, 'grad_norm': 0.6624971032142639, 'learning_rate': 7e-05, 'epoch': 6.5}\n",
      "{'loss': 3.2838, 'grad_norm': 0.5322293043136597, 'learning_rate': 6e-05, 'epoch': 7.0}\n",
      "{'loss': 3.2859, 'grad_norm': 0.6307340860366821, 'learning_rate': 5e-05, 'epoch': 7.5}\n",
      "{'loss': 3.4579, 'grad_norm': 0.7142120003700256, 'learning_rate': 4e-05, 'epoch': 8.0}\n",
      "{'loss': 3.0286, 'grad_norm': 0.46664878726005554, 'learning_rate': 3e-05, 'epoch': 8.5}\n",
      "{'loss': 3.2563, 'grad_norm': 1.101199984550476, 'learning_rate': 2e-05, 'epoch': 9.0}\n",
      "{'loss': 3.3074, 'grad_norm': 0.6449880599975586, 'learning_rate': 1e-05, 'epoch': 9.5}\n",
      "{'loss': 3.2288, 'grad_norm': 0.9561487436294556, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105dfd6bcae94b63be8425c58982b962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3711564540863037, 'eval_runtime': 0.0392, 'eval_samples_per_second': 51.028, 'eval_steps_per_second': 25.514, 'epoch': 10.0}\n",
      "{'train_runtime': 3.3759, 'train_samples_per_second': 17.773, 'train_steps_per_second': 5.924, 'train_loss': 3.283142387866974, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "# from datasets import load_dataset  # 使用 Hugging Face 資料集（可選）\n",
    "\n",
    "# 抑制警告（可選，保持輸出乾淨）\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 載入預訓練模型和分詞器\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    clean_up_tokenization_spaces=True  # 抑制 FutureWarning\n",
    ")\n",
    "\n",
    "# 設置填充標記\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 定義 LoRA 配置\n",
    "# 根據 GPT-2 架構調整 target_modules（注意力層）\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,  # 增加 rank 以提升表現（可調整）\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"h.0.attn.c_attn\", \"h.1.attn.c_attn\", \"h.0.attn.c_proj\", \"h.1.attn.c_proj\"],  # 擴展至更多層\n",
    ")\n",
    "\n",
    "# 應用 PEFT 模型\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 擴展或使用更大的訓練資料集\n",
    "# 範例資料（可替換為真實資料集）\n",
    "train_data = [\n",
    "    \"Q: What is 2 + 2? Let's think step by step. First, we know that 2 is a number. Adding another 2 to it gives us 4.\",\n",
    "    \"Q: What is the capital of France? Let's think step by step. France is a country in Europe. The capital of France is Paris.\",\n",
    "    \"Q: If a train leaves the station at 3 PM and travels at 60 mph, what time will it reach a station 180 miles away? Let's think step by step. The train travels 60 miles in one hour. To travel 180 miles, it will take 3 hours. Therefore, it will reach the station at 6 PM.\",\n",
    "    \"Q: What is 5 + 3? Let's think step by step. Starting with 5, adding 3 more gives us 8.\",\n",
    "    \"Q: What is the capital of Japan? Let's think step by step. Japan is an island nation in Asia. Its capital is Tokyo.\",\n",
    "    \"Q: If a car drives at 40 mph for 120 miles, how long does it take? Let's think step by step. The car travels 40 miles in one hour. For 120 miles, it takes 120 / 40 = 3 hours.\"\n",
    "]\n",
    "\n",
    "# 自訂資料集類別\n",
    "class ReasoningDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(\n",
    "            self.data[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()  # 為因果語言模型添加標籤\n",
    "        return inputs\n",
    "\n",
    "# 創建資料集和 DataLoader\n",
    "train_dataset = ReasoningDataset(train_data, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 定義訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=10,  # 增加 epoch 數以適應 LoRA 訓練\n",
    "    learning_rate=2e-4,  # 為 LoRA 設定適當的學習率\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,  # 更頻繁地記錄進度\n",
    "    evaluation_strategy=\"steps\",  # 添加評估策略\n",
    "    eval_steps=10,  # 每 10 步評估一次\n",
    "    disable_tqdm=False  # 確保進度條啟用（若環境支援）\n",
    ")\n",
    "\n",
    "# 數據整理器\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 創建驗證資料集（範例）\n",
    "val_data = [\n",
    "    \"Q: What is 4 + 4? Let's think step by step. 4 plus 4 is 8.\",\n",
    "    \"Q: What is the capital of Germany? Let's think step by step. Germany is in Europe. Its capital is Berlin.\"\n",
    "]\n",
    "val_dataset = ReasoningDataset(val_data, tokenizer)\n",
    "\n",
    "# Trainer 實例\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  # 添加驗證資料集\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# 訓練模型\n",
    "trainer.train()\n",
    "\n",
    "# 儲存微調後的模型\n",
    "peft_model.save_pretrained(\"./fine_tuned_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Q: What is 2 + 2? Let's think step by step.\n",
      "Response: Q: What is 2 + 2? Let's think step by step.\n",
      "\n",
      "S: It's 2.\n",
      "\n",
      "N: And now you're going to see it.\n",
      "\n",
      "S: Actually, let's say you're a doctor.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# 載入分詞器和基礎模型\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 確保填充標記設置\n",
    "\n",
    "# 載入基礎模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 載入 LoRA 配置和適配器\n",
    "config = PeftConfig.from_pretrained(\"./fine_tuned_lora_model\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./fine_tuned_lora_model\")\n",
    "\n",
    "# 設置模型為評估模式\n",
    "model.eval()\n",
    "\n",
    "# 定義推理函數\n",
    "def generate_response(prompt, max_length=50):\n",
    "    # 將輸入轉換為模型可用的格式\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # 生成輸出\n",
    "    with torch.no_grad():  # 禁用梯度計算\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,  # 使用隨機採樣生成\n",
    "            temperature=0.7,  # 控制生成的多樣性\n",
    "            top_k=50,  # 控制生成時的候選詞數\n",
    "        )\n",
    "    \n",
    "    # 解碼輸出\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# 測試推理\n",
    "prompt = \"Q: What is 2 + 2? Let's think step by step.\"\n",
    "response = generate_response(prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Q: What is 2 + 2? Let's think step by step.\\n\\nA: 2 + 2 = 2 + 2 = 2 + 2 = 2 + 2 = 2 + 2 = 2 + 2 = 2 + 2 = 2 + 2\",\n",
       " 'Q: What is the capital of France?\\n\\nA: The capital of France is Paris.\\n\\nQ: What is the capital of France?\\n\\nA: The capital of France is Paris.\\n\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\"Q: What is 2 + 2? Let's think step by step.\", \"Q: What is the capital of France?\"]\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "responses = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
