{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jhbai\\Anaconda\\envs\\MachineLearningEnv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\jhbai\\Anaconda\\envs\\MachineLearningEnv\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已套用 LoRA，總參數數量: 124734720\n",
      "開始訓練，使用 self-motivated learning 與 PPO 機制...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8170f59deb480bbdda24a81f721144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8.3324, 'train_samples_per_second': 0.72, 'train_steps_per_second': 0.48, 'train_loss': 2.6507771015167236, 'epoch': 2.0}\n",
      "微調完成，模型儲存於 ./lora_self_motivated_finetune/final_model\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "範例：GPT-2 + LoRA + 自定義 PPO 損失 (Self-motivated Learning)\n",
    "修正：將 generate() 的 max_length 改為 max_new_tokens，以免 prompt 長度超過 max_length。\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 準備自定義 Dataset\n",
    "# -----------------------------\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = []\n",
    "        for item in data_list:\n",
    "            q = str(item.get(\"question\", \"\"))\n",
    "            a = str(item.get(\"answer\", \"\"))\n",
    "            # 若要過濾空值，可自行加判斷\n",
    "            if len(q.strip()) > 0 and len(a.strip()) > 0:\n",
    "                self.data_list.append({\"question\": q, \"answer\": a})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "def qa_data_collator(features):\n",
    "    batch_questions = [f[\"question\"] for f in features]\n",
    "    batch_answers = [f[\"answer\"] for f in features]\n",
    "    return {\"question\": batch_questions, \"answer\": batch_answers}\n",
    "\n",
    "data_list = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote the play Romeo and Juliet?\", \"answer\": \"William Shakespeare\"},\n",
    "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
    "]\n",
    "train_dataset = QADataset(data_list)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. 載入預訓練模型與 Tokenizer\n",
    "# -----------------------------\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 設定 LoRA 微調參數，並套用至模型\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"模型已套用 LoRA，總參數數量:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# -----------------------------\n",
    "# 4. 定義簡易 Reward 與 PPO Loss\n",
    "# -----------------------------\n",
    "def simple_reward_function(generated_answer, ground_truth):\n",
    "    return 1.0 if generated_answer.strip().lower() == ground_truth.strip().lower() else 0.0\n",
    "\n",
    "def compute_ppo_loss(model, question, ground_truth, tokenizer, generation_kwargs):\n",
    "    # 先生成 reasonale\n",
    "    input_ids = tokenizer.encode(question, return_tensors=\"pt\").to(model.device)\n",
    "    cot_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=50,  # 改用 max_new_tokens\n",
    "        do_sample=True,\n",
    "        **generation_kwargs\n",
    "    )\n",
    "    reasonale = tokenizer.decode(cot_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # 組成 prompt: [question + reasonale]\n",
    "    prompt = question + \"\\n\" + reasonale\n",
    "    prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    answer_ids = model.generate(\n",
    "        prompt_ids,\n",
    "        max_new_tokens=50,  # 改用 max_new_tokens\n",
    "        do_sample=True,\n",
    "        **generation_kwargs\n",
    "    )\n",
    "    generated_answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # 計算 reward\n",
    "    reward = simple_reward_function(generated_answer, ground_truth)\n",
    "\n",
    "    # 計算交叉熵損失\n",
    "    outputs = model(prompt_ids, labels=prompt_ids)\n",
    "    ce_loss = outputs.loss\n",
    "\n",
    "    # PPO (示意)\n",
    "    baseline = 0.5\n",
    "    advantage = reward - baseline\n",
    "    epsilon = 0.2\n",
    "    ratio = torch.tensor(1.0).to(model.device)\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "    ppo_loss = -torch.min(ratio * advantage, clipped_ratio * advantage)\n",
    "\n",
    "    total_loss = ce_loss + ppo_loss\n",
    "    return total_loss\n",
    "\n",
    "# -----------------------------\n",
    "# 5. 自定義 Trainer\n",
    "# -----------------------------\n",
    "class SelfMotivatedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        questions = inputs[\"question\"]\n",
    "        answers = inputs[\"answer\"]\n",
    "\n",
    "        generation_kwargs = {\"temperature\": 0.7, \"top_p\": 0.9}\n",
    "\n",
    "        total_loss = 0.0\n",
    "        batch_size = len(questions)\n",
    "        for q, a in zip(questions, answers):\n",
    "            total_loss += compute_ppo_loss(model, str(q), str(a), tokenizer, generation_kwargs)\n",
    "\n",
    "        final_loss = total_loss / batch_size\n",
    "        return (final_loss, None) if return_outputs else final_loss\n",
    "\n",
    "# -----------------------------\n",
    "# 6. 定義訓練參數\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_self_motivated_finetune\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=5,\n",
    "    save_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. 建立 Trainer 實例並訓練\n",
    "# -----------------------------\n",
    "trainer = SelfMotivatedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=qa_data_collator,\n",
    ")\n",
    "\n",
    "print(\"開始訓練，使用 self-motivated learning 與 PPO 機制...\")\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------------\n",
    "# 8. 儲存模型與 Tokenizer\n",
    "# -----------------------------\n",
    "trainer.save_model(\"./lora_self_motivated_finetune/final_model\")\n",
    "tokenizer.save_pretrained(\"./lora_self_motivated_finetune/final_model\")\n",
    "\n",
    "print(\"微調完成，模型儲存於 ./lora_self_motivated_finetune/final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " What is the capital of France?\n",
      "The reasonale is that it is the most comprehensive of all the capital markets in the world. It's the only one in which the capital is concentrated in the hands of one group of investors. It's the only one which has a capital of its own.\n",
      "We believe that capital can be made up of all kinds of things, including all sorts of things that could be made up of different kinds of capital. That's what we have in the capital market. We believe that it's the best capital in the world.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. 指定微調後模型的路徑\n",
    "lora_model_path = \"./lora_self_motivated_finetune/final_model\"\n",
    "\n",
    "# 2. 載入 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_model_path)\n",
    "\n",
    "# 3. 載入原始 base model（與你微調時用的相同，如 \"gpt2\"）\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 4. 利用 PEFT 的 API，將 LoRA 權重載入 base model\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "\n",
    "# 5. 切換到 GPU（若可用）或保持在 CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 6. 測試：做一個簡單的推理\n",
    "prompt = \"What is the capital of France?\\nThe reasonale is that\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 這裡示範用 max_new_tokens，而非 max_length，避免 prompt 太長時的錯誤\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,     # 是否使用隨機采樣\n",
    "    top_p=0.9,          # nucleus sampling\n",
    "    temperature=0.7     # 溫度\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
